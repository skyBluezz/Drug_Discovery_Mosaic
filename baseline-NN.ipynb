{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport time\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/lish-moa/train_targets_scored.csv\n/kaggle/input/lish-moa/sample_submission.csv\n/kaggle/input/lish-moa/train_drug.csv\n/kaggle/input/lish-moa/train_targets_nonscored.csv\n/kaggle/input/lish-moa/train_features.csv\n/kaggle/input/lish-moa/test_features.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\ntarget = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\ntest = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n# train_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\ntrain_drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\nprint('read all the data into pandas')","execution_count":29,"outputs":[{"output_type":"stream","text":"read all the data into pandas\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_columns = set(train.columns)\nprint(len(all_columns))\nremove_columns = set(['sig_id', 'cp_type', 'cp_time', 'cp_dose'])\ntrain_columns = all_columns - remove_columns\nprint(len(train_columns))\ntarget_columns = set(target.columns) - set(['sig_id'])\nprint(len(target_columns))\n","execution_count":3,"outputs":[{"output_type":"stream","text":"876\n872\n206\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def z_Score_norm(train):\n  GENES = [col for col in train.columns if col.startswith('g-')]\n  CELLS = [col for col in train.columns if col.startswith('c-')]\n  control_data = train[train['cp_type'] == 0]\n  control_24 = control_data[control_data['cp_time'] == 0] ##Control data for cp_time = 24\n  control_48 = control_data[control_data['cp_time'] == 1]\n  control_72 = control_data[control_data['cp_time'] == 2]\n\n  for col in (GENES+CELLS):\n    train.loc[train['cp_time'] == 0, col] -= control_24[col].mean()\n    train.loc[train['cp_time'] == 0, col] /= control_24[col].std()\n\n    train.loc[train['cp_time'] == 1, col] -= control_48[col].mean()\n    train.loc[train['cp_time'] == 1, col] /= control_48[col].std()\n\n    train.loc[train['cp_time'] == 2, col] -= control_72[col].mean()\n    train.loc[train['cp_time'] == 2, col] /= control_72[col].std()\n\n  return train","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size,dtype):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        \n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(train_data,val_data,model,learning_rate,weight_decay):\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=num_epochs, steps_per_epoch=len(train_data))\n    # enumerate epochs\n    for epoch in range(num_epochs):\n        # enumerate mini batches\n        epoch_loss = 0\n        start_time = time.time()\n        for i, (x, y) in enumerate(train_data):\n            optimizer.zero_grad()\n            y_pred = model(x)\n            loss = loss_fn(y,y_pred)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n            epoch_loss += loss\n        epoch_loss /= i\n        epoch_time = (time.time() - start_time)/60\n        print(f\"EPOCH: {epoch}, train_loss: {epoch_loss}, in time{epoch_time}\")\n    \n    # validation set\n    # check validation accuracy\n    # model.eval()\n    final_val_loss = 0\n    for i,(x,y) in enumerate(val_data):\n        y_pred = model(x)\n        loss = loss_fn(y,y_pred)\n        final_val_loss += loss\n    final_val_loss /= i\n    print(f\" validation loss {final_val_loss}\")\n    return model\n\ndef test_model(data, model):      \n    model.eval()\n    with torch.no_grad():\n        outputs = model(data)\n    return outputs\n","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nz_Score_normalize = True\n\nif z_Score_normalize == True:\n    train = z_Score_norm(train)\n\n\ntrain_tensor = torch.tensor(train[train_columns].to_numpy()).double()\ntarget_tensor = torch.tensor(target[target_columns].to_numpy()).double() #,dtype=torch.double\ntest_tensor = torch.tensor(test[train_columns].to_numpy()).double()\n\n# hyperparameters\nbatch_size  = 1024\nhidden_size = 1024\nlearning_rate = 1e-3\nweight_decay = 1e-5\nnum_epochs = 50\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device, {DEVICE}\")\ntrain_tensor = train_tensor.to(DEVICE)\ntarget_tensor = target_tensor.to(DEVICE)\ntest_tensor = test_tensor.to(DEVICE)\nprint(train_tensor.device,target_tensor.device)\n\n\n\ndataset = TensorDataset(train_tensor.double(),target_tensor)\ntrain_set, val_set = torch.utils.data.random_split(dataset, [20000,train.shape[0]-20000])\ntrain_data = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_data = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n\n\nmodel = Model(len(train_columns),len(target_columns),hidden_size,dtype)\nmodel.to(DEVICE).to(dtype)\n# model.double()\n\n\n\nmodel = train_model(train_data,val_data,model,learning_rate,weight_decay)\n# predictions = test_model(test_tensor,model)","execution_count":30,"outputs":[{"output_type":"stream","text":"Device, cuda\ncuda:0 cuda:0\nEPOCH: 0, train_loss: 0.6499016161676551, in time0.01141664187113444\nEPOCH: 1, train_loss: 0.430498963540519, in time0.009739073117574056\nEPOCH: 2, train_loss: 0.2986098134614814, in time0.010731132825215657\nEPOCH: 3, train_loss: 0.1743147601248169, in time0.009856406847635906\nEPOCH: 4, train_loss: 0.08849886385633682, in time0.009621357917785645\nEPOCH: 5, train_loss: 0.04932325809972628, in time0.010416905085245768\nEPOCH: 6, train_loss: 0.030418225979343644, in time0.009437835216522217\nEPOCH: 7, train_loss: 0.0204020384976694, in time0.02308080593744914\nEPOCH: 8, train_loss: 0.014711150215626037, in time0.009365181128184\nEPOCH: 9, train_loss: 0.010948207414112945, in time0.009827121098836263\nEPOCH: 10, train_loss: 0.007946338970919092, in time0.010471606254577636\nEPOCH: 11, train_loss: 0.0062331557732278236, in time0.009432733058929443\nEPOCH: 12, train_loss: 0.005170618463057024, in time0.009882330894470215\nEPOCH: 13, train_loss: 0.004495980429331848, in time0.010575528939565022\nEPOCH: 14, train_loss: 0.004079989951312126, in time0.010315207640329997\nEPOCH: 15, train_loss: 0.003808342566726541, in time0.010445606708526612\nEPOCH: 16, train_loss: 0.003616665251917062, in time0.009323660532633464\nEPOCH: 17, train_loss: 0.003505410526703535, in time0.009351042906443278\nEPOCH: 18, train_loss: 0.003427551766531634, in time0.01058648427327474\nEPOCH: 19, train_loss: 0.0033714590282608223, in time0.00935906966527303\nEPOCH: 20, train_loss: 0.003329144586102468, in time0.010474789142608642\nEPOCH: 21, train_loss: 0.0033096108543142926, in time0.009366591771443685\nEPOCH: 22, train_loss: 0.0032987310672804872, in time0.009706449508666993\nEPOCH: 23, train_loss: 0.003278779193726551, in time0.011221758524576823\nEPOCH: 24, train_loss: 0.0032662399020922813, in time0.01333382527033488\nEPOCH: 25, train_loss: 0.0032693030749166067, in time0.009798030058542887\nEPOCH: 26, train_loss: 0.0032603651173636573, in time0.010392280419667561\nEPOCH: 27, train_loss: 0.003250505871166081, in time0.009186371167500814\nEPOCH: 28, train_loss: 0.0032468314084851736, in time0.014879910151163737\nEPOCH: 29, train_loss: 0.0032435633528255813, in time0.011269462108612061\nEPOCH: 30, train_loss: 0.00323408180090368, in time0.00912713607152303\nEPOCH: 31, train_loss: 0.0032335260231236428, in time0.0103761355082194\nEPOCH: 32, train_loss: 0.0032311045590430565, in time0.009308850765228272\nEPOCH: 33, train_loss: 0.0032260701113431757, in time0.010258936882019043\nEPOCH: 34, train_loss: 0.003221493529090679, in time0.009100619951883953\nEPOCH: 35, train_loss: 0.003233805666769606, in time0.00914980173110962\nEPOCH: 36, train_loss: 0.0032262600081438752, in time0.01031724214553833\nEPOCH: 37, train_loss: 0.0032316222012723703, in time0.009117074807484945\nEPOCH: 38, train_loss: 0.003215278630850323, in time0.010486749807993571\nEPOCH: 39, train_loss: 0.0032195200330899956, in time0.009259724617004394\nEPOCH: 40, train_loss: 0.0032135526496400134, in time0.009103469053904216\nEPOCH: 41, train_loss: 0.003204735286671879, in time0.012565330664316813\nEPOCH: 42, train_loss: 0.0032092795789376715, in time0.011542002360026041\nEPOCH: 43, train_loss: 0.0032111017532843343, in time0.009327606360117594\nEPOCH: 44, train_loss: 0.0032064117710974, in time0.010217829545338949\nEPOCH: 45, train_loss: 0.003207624856377665, in time0.009179437160491943\nEPOCH: 46, train_loss: 0.0032025497283459177, in time0.009179747104644776\nEPOCH: 47, train_loss: 0.003202867983774995, in time0.010408719380696615\nEPOCH: 48, train_loss: 0.0032003710400228696, in time0.009246416886647542\nEPOCH: 49, train_loss: 0.003204031029865266, in time0.010257792472839356\n validation loss 0.004124919414228635\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation loss 0.004120627779762144","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame(predictions.numpy())\nsubmit.columns = list(target_columns)\nx_test_id = test['sig_id']\nsubmit = pd.concat([x_test_id,submit],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import log_loss\n\n# xx,yy = train_set[:]\n# y_pred = test_model(xx,model)\n# ll = log_loss(yy,y_pred)\n# print(ll)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_drug['sig_id']=='id_008a986b7'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train_drug['sig_id']=='id_008a986b7']['cp_type']=='ctl_vehicle'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t2 = train_tensor.to(DEVICE)\nprint(t2.device)","execution_count":21,"outputs":[{"output_type":"stream","text":"cuda:0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cp_type']=='ctl_vehicle'","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}